# Data Warehouse

In this section, I would build and run ETL pipelines using DLT, which is a Python package, to load Parquet files from the NYC_TLC website to Google Cloud Storage. After that, I used SQL queries in Big Query to create tables.

`Upload2GCS_DLT.ipynb` - this file was used to build an ETL pipeline with DLT. This package is very helpful as it will handle all the errors that can happen when running the pipeline.
