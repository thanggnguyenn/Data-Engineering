{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "suF0-Vl2viSf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"DESTINATION__CREDENTIALS\"] = userdata.get('GCP_CREDENTIALS')\n",
        "os.environ[\"BUCKET_URL\"] = \"YOUR_BUCKET_NAME_HERE\" # YOUR_BUCKET_NAME_HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install for production\n",
        "%%capture\n",
        "!pip install dlt[bigquery, gs]"
      ],
      "metadata": {
        "id": "nOMLyLU0wM3u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install for testing\n",
        "%%capture\n",
        "!pip install dlt[duckdb]"
      ],
      "metadata": {
        "id": "Hinhmo0WwP3G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dlt\n",
        "import requests\n",
        "import pandas as pd\n",
        "from dlt.destinations import filesystem\n",
        "from io import BytesIO"
      ],
      "metadata": {
        "id": "huhbS2BiwSYU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. You can easily implement your own sources, as long as you yield data in a way that is compatible with dlt, such as JSON objects, Python lists and dictionaries, pandas dataframes, and arrow tables.\n",
        "\n",
        "2. A pipeline is a connection that moves data from your Python code to a destination. The pipeline accepts dlt sources or resources.\n",
        "\n"
      ],
      "metadata": {
        "id": "fkiGMcRXBn6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dlt source to download and process Parquet files as resources\n",
        "@dlt.source(name=\"rides\")\n",
        "def download_parquet():\n",
        "     for month in range(1,7):\n",
        "      file_name = f\"yellow_tripdata_2024-0{month}.parquet\"\n",
        "\n",
        "      url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-0{month}.parquet\"\n",
        "      response = requests.get(url)\n",
        "\n",
        "      df = pd.read_parquet(BytesIO(response.content))\n",
        "\n",
        "      # Return the dataframe as a dlt resource for ingestion\n",
        "      yield dlt.resource(df, name=file_name)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"rides_pipeline\",\n",
        "    destination=filesystem(\n",
        "      layout=\"{schema_name}/{table_name}.{ext}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Run the pipeline to load Parquet data into DuckDB\n",
        "load_info = pipeline.run(\n",
        "    download_parquet(),\n",
        "    loader_file_format=\"parquet\"\n",
        "    )\n",
        "\n",
        "# Print the results\n",
        "print(load_info)"
      ],
      "metadata": {
        "id": "BVcgpPfuwV3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lb8UDPGEKVFh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
